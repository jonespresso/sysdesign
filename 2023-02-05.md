# Mussel - Airbnb's key-value store for derived data
## Requirements
1. Scale to petabytes of data
2. Efficient bulk load
3. Low latency reads (<50ms P99)
4. Multi-tenant storage service that can be used by multiple customers

## Stage 1 HFileService
Build HFileService using HFile (building block of Hadoop HBase). Transform data with daily Hadoop job and upload to S3. Servers download transformed data from S3. Data are sharded by primary keys. User -> get hash modulo number of shards -> query Zookeeper to get a list of servers with the shards -> request one of them.

## Stage 2 Nebula
Need to support point, low-latency writes. To support real-time data DynamoDB is used. To support batch-update data, S3/HFile is used. Time based versioning is introduced. Data are read from dynamic tables and static snapshots and merged based on timestamps.<br/>
Merge operations are performed with daily scheduled spark jobs.

## Stage 3 Mussel
To overcome scale-out challenge, improve read performance under spiky write traffic, reduce the maintainence overhead for both DynamoDB and HFileService, and improve the merging process efficiency, a new key-value store system is introduced:<br/>
- Use Helix to manage partition mapping
- Leverage Kafka to replicate writes to all replicas
- Use HRegion as the only storage engine in the storage nodes
- Use a Spark pipeline to load data from the data warehouse into storage nodes directly
### Manage partitions with Helix
Number of shards: 8 -> 1024. Helix manage mapping of logical shards to physical storage nodes. Each node has multiple logical shards and each shard is replicated across multiple nodes.
### Leaderless replication with Kafka
Adopt leader-less architecture because Mussel is a read-heavy store. Read requests can be served by any node that contains the logical shard.
- Need to smooth writes to avoid impact on the read path
- Need to ensure consistency between nodes for each shard
Kafka is used as a write-ahead-log. It has 1024 partitions, each for a logical shard. This only provides eventual consistency but the tradeoff is good enough for the derived data use case.<br/>
Use HRegion instead of HFile to better support both read and write operations. HRegion is a fully functional key-value store with MemStore and BlockCache. Internally it uses a Log Structured Merged (LSM) Tree.